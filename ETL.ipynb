{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0322a-5097-4ee6-b832-2a49670807b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import IntegrityError\n",
    "import urllib\n",
    "import traceback\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24793b-8032-4003-b4cb-25d8a69697ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load CSV files into the bronze layer\n",
    "params = urllib.parse.quote_plus(r'Driver={ODBC Driver 17 for SQL Server};Server=tcp:{SQL},1433;Database=demo;UID={UID};PWD={PWD};Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;')\n",
    "conn_str = 'mssql+pyodbc:///?odbc_connect={}'.format(params)\n",
    "conn = create_engine(conn_str,echo=True)\n",
    "\n",
    "def log_load (table_name,file_name,row_count,elt_load_timestamp) :\n",
    "    df2 = pd.DataFrame({\n",
    "    'table_name':[f'{table_name}'],\n",
    "    'file_name':[f'{file_name}'],\n",
    "    'row_count':[f'{row_count}'],\n",
    "    'elt_load_timestamp' : [f'{elt_load_timestamp}']\n",
    "    })\n",
    "    df2.to_sql('elt_load_table',conn, if_exists='append', index=False, schema = 'aux')\n",
    "\n",
    "def data_profiling(df, filename):\n",
    "    profiling_info = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        data_type = str(df[col].dtype)\n",
    "        unique_values = df[col].nunique()\n",
    "        missing_values = df[col].isnull().sum()\n",
    "        \n",
    "        inconsistent_values = []\n",
    "        first_value_type = df[col].dropna().iloc[0].__class__\n",
    "        for value in df[col]:\n",
    "            if not isinstance(value, first_value_type):\n",
    "                inconsistent_values.append(str(value))\n",
    "        \n",
    "        profiling_info.append({'filename': filename, 'Column': col, 'Data_Type': data_type, 'Unique_Values': unique_values, 'Missing_Values': missing_values, 'Inconsistent_Value': '::'.join(inconsistent_values)})\n",
    "    \n",
    "    pd.DataFrame(profiling_info).to_sql('data_profiling', conn, if_exists='append', index=False, schema='aux')\n",
    "\n",
    "\n",
    "def insert_records(df,table_name,conn,schema):\n",
    "    try:\n",
    "        df.to_sql(table_name, conn, index=False, if_exists='append', schema= schema)\n",
    "    except Exception as e:\n",
    "        # Log the error message and traceback\n",
    "        error_message = str(e)\n",
    "        error_traceback = traceback.format_exc()\n",
    "        # Insert the failed record into the error table\n",
    "        error_df = pd.DataFrame({'error_message': [error_message]})\n",
    "        error_df = pd.concat([error_df, df], axis=1)\n",
    "        error_df.to_sql(table_name + '_error_log', conn, index=False, if_exists='append', schema='aux')\n",
    "\n",
    "\n",
    "#cleaning columns and generating surrogate_keys\n",
    "def preprocess_string(string_or_int):\n",
    "    if isinstance(string_or_int, int):\n",
    "        string_or_int = str(string_or_int)\n",
    "    return string_or_int.lower().strip().replace('-', '')\n",
    "\n",
    "def consistent_hash(string_or_int, limit):\n",
    "    preprocessed_string = preprocess_string(string_or_int)\n",
    "    hash_object = hashlib.sha256(preprocessed_string.encode())\n",
    "    hash_integer = int(hash_object.hexdigest(), 16)\n",
    "    return hash_integer % limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4e997-eb95-47bb-bb1c-d8383dd761b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV files to bronze layer\n",
    "def load_csv_files(csv_dir):\n",
    "    log_table_query = \"SELECT DISTINCT file_name FROM aux.elt_load_table\"\n",
    "    loaded_files = pd.read_sql(log_table_query, conn)['file_name'].tolist()\n",
    "    # Iterate through CSV files in the directory\n",
    "    for filename in os.listdir(csv_dir):\n",
    "        if filename.endswith('.csv') and filename not in loaded_files:\n",
    "            file_path = os.path.join(csv_dir, filename)\n",
    "            df = pd.read_csv(file_path,encoding='cp1252', delimiter='|')\n",
    "            data_profiling(df,filename)\n",
    "            # Extract date from the file name and make it as monthend\n",
    "            date_str = filename.split('_')[0]\n",
    "            end_of_month_date = pd.to_datetime(date_str, format='%Y%m') + pd.offsets.MonthEnd(0)\n",
    "            datetime_str = filename.split('_Orders_')[1].split('.csv')[0]\n",
    "            filecreationtimestamp = pd.to_datetime(datetime_str, format='%Y_%m_%d_%H_%M_%S')\n",
    "            time_now = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            #Adding Audit Columns in filename\n",
    "            df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "            df['elt_file_date'] = end_of_month_date.date()\n",
    "            df['elt_file_loading_timestamp'] = time_now\n",
    "            df['elt_file_timestamp'] = filecreationtimestamp\n",
    "            # Load DataFrame into the bronze layer table\n",
    "            try:\n",
    "                df.to_sql('orders', conn, index=False, if_exists='append',schema='bronze')\n",
    "                log_load('orders',filename,str(len(df)),time_now)\n",
    "            except pyodbc.DataError as e:\n",
    "                error_message = str(e)\n",
    "                if 'data truncation' in error_message.lower() or 'data type mismatch' in error_message.lower():\n",
    "                    # Log the data truncation or data type mismatch error\n",
    "                    print(f\"Data truncation or data type mismatch error occurred in file: {filename}\")\n",
    "                    # Log the error message\n",
    "                    print(f\"Error message: {error_message}\")\n",
    "                    # Log the number of rows causing the error\n",
    "                    print(f\"Number of rows causing the error: {len(df)}\")\n",
    "                    # Insert the failing records into the error table\n",
    "                    error_df = df.copy()  # Create a copy of the DataFrame\n",
    "                    error_df['error_message'] = error_message  # Add a column for the error message\n",
    "                    error_df['elt_file_name'] = filename  # Add a column for the file name\n",
    "                    error_df.to_sql('error_table', conn, index=False, if_exists='append', schema='aux')\n",
    "                    print(\"Failing records inserted into error table.\")\n",
    "                else:\n",
    "                    # Handle other data-related errors\n",
    "                    print(f\"Data-related error occurred in file: {filename}\")\n",
    "                    print(f\"Error message: {error_message}\")\n",
    "            except Exception as e:\n",
    "                # Handle other exceptions\n",
    "                print(f\"Error occurred while processing file: {filename}\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "\n",
    "            \n",
    "# Directory containing CSV files\n",
    "csv_dir = r'{Source_Location}'",
    "load_csv_files(csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876851ee-f690-47d8-92b6-f66a9811afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "import json\n",
    "query = \"SELECT * FROM demo.bronze.orders_bkp;\"\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a4c9a-1b34-419d-bcbc-419e70b21384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ge = ge.dataset.PandasDataset(df)\n",
    "uniqueness = df_ge.expect_compound_columns_to_be_unique([\"row_id\",\"elt_file_timestamp\"],result_format={\"result_format\": \"COMPLETE\",\"unexpected_index_column_names\": [\"row_id\"],\"return_unexpected_index_query\": True})\n",
    "value_not_null = df_ge.expect_column_values_to_not_be_null('row_id',result_format={\"result_format\": \"COMPLETE\",\"unexpected_index_column_names\": [\"row_id\"],\"return_unexpected_index_query\": True})\n",
    "order_date = df_ge.expect_column_values_to_match_regex(column='order_date', regex=r'\\d{2}-\\d{2}-\\d{4}', result_format={\"result_format\": \"COMPLETE\",\"unexpected_index_column_names\": [\"row_id\"],\"return_unexpected_index_query\": True})\n",
    "ship_date = df_ge.expect_column_values_to_match_regex(column='ship_date', regex=r'\\d{2}-\\d{2}-\\d{4}', result_format={\"result_format\": \"COMPLETE\",\"unexpected_index_column_names\": [\"row_id\"],\"return_unexpected_index_query\": True})\n",
    "customer_id = df_ge.expect_column_values_to_match_regex(column='customer_id', regex=r'^[A-Za-z]{2}-\\d{5}$', result_format={\"result_format\": \"COMPLETE\",\"unexpected_index_column_names\": [\"row_id\"],\"return_unexpected_index_query\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0b0d5-7c57-4e25-ab26-f877b46d196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_expectations.data_context.types.resource_identifiers import ValidationResultIdentifier\n",
    "\n",
    "context = ge.data_context.DataContext()\n",
    "expectation_suite = df_ge.get_expectation_suite()\n",
    "context.save_expectation_suite(expectation_suite)\n",
    "validation_results = df_ge.validate()\n",
    "\n",
    "context.build_data_docs()\n",
    "print(context.get_docs_sites_urls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecff696-1ad9-4b4c-8bf2-7c56d3da3a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import text\n",
    "\n",
    "# Load Customer Data\n",
    "query = \"\"\"\n",
    "    WITH CUST AS (\n",
    "        SELECT DISTINCT \n",
    "            customer_id,\n",
    "            customer_name,\n",
    "            segment,\n",
    "            elt_file_date,\n",
    "            ROW_NUMBER () OVER (PARTITION BY TRIM(customer_id), TRIM(customer_name) ORDER BY elt_file_date DESC) RK \n",
    "        FROM demo.bronze.orders\n",
    "    ) \n",
    "    SELECT \n",
    "        TRIM(customer_id) customer_id,\n",
    "        TRIM(customer_name) customer_name,\n",
    "        TRIM(segment) segment,\n",
    "        elt_file_date \n",
    "    FROM CUST \n",
    "    WHERE RK = 1\n",
    "\"\"\"\n",
    "customers = pd.read_sql(query, conn)\n",
    "\n",
    "query = \"SELECT * FROM silver.customer\"\n",
    "existing_customers = pd.read_sql(query, conn)\n",
    "existing_customers_filtered = existing_customers.copy()\n",
    "\n",
    "# Iterate through each customer\n",
    "for index, customer in customers.iterrows():\n",
    "    # Check if the customer exists in the database\n",
    "    existing_customer = existing_customers_filtered[existing_customers_filtered['customer_id'] == customer['customer_id']]\n",
    "    if len(existing_customer) == 0:\n",
    "        # If the customer doesn't exist, insert a new record\n",
    "        sk_customer_id = consistent_hash(customer['customer_id'], 10 ** 9)\n",
    "        segment_key = consistent_hash(customer['segment'], 10 ** 9)\n",
    "        valid_from = customer['elt_file_date']\n",
    "        valid_to = datetime.strptime('9999-01-01', '%Y-%m-%d').strftime('%Y-%m-%d')\n",
    "        his_flag = 'C'\n",
    "        elt_load_time = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        new_customer_data = {\n",
    "            'sk_customer_id': sk_customer_id,\n",
    "            'segment_key': segment_key,\n",
    "            'customer_id': customer['customer_id'],\n",
    "            'customer_name': customer['customer_name'],\n",
    "            'valid_from': valid_from,\n",
    "            'valid_to': valid_to,\n",
    "            'his_flag': his_flag,\n",
    "            'elt_load_time': elt_load_time\n",
    "        }\n",
    "        \n",
    "        # Append new customer data to DataFrame and insert into database\n",
    "        new_customer_df = pd.DataFrame([new_customer_data])\n",
    "        #new_customer_df.to_sql('customer', conn, index=False, if_exists='append', schema='silver')\n",
    "        insert_records(new_customer_df, 'customer', conn,'silver')\n",
    "    else:\n",
    "        # If the customer exists, check for changes\n",
    "        existing_customer = existing_customer.iloc[0] \n",
    "        \n",
    "        if (customer['customer_name'].lower() != existing_customer['customer_name'].lower()) or ((hash(customer['segment']) % (10 ** 9)) != existing_customer['segment_key']):\n",
    "            # Changes detected, end-date existing record and insert new record\n",
    "            valid_to = customer['elt_file_date']\n",
    "            his_flag = 'H' \n",
    "            \n",
    "            # Update existing record in the database\n",
    "            update_query = f\"\"\"\n",
    "                UPDATE silver.customer \n",
    "                SET valid_to = '{valid_to}', his_flag = '{his_flag}' \n",
    "                WHERE customer_id = '{customer['customer_id']}' AND his_flag = 'C'\n",
    "            \"\"\"\n",
    "            with conn.connect() as connection:\n",
    "                connection.execute(text(update_query))\n",
    "                connection.commit()\n",
    "            \n",
    "            # Insert new record into the database\n",
    "            new_customer_data = {\n",
    "                'sk_customer_id': consistent_hash(customer['customer_id'], 10 ** 9),\n",
    "                'segment_key': consistent_hash(customer['segment'], 10 ** 9),\n",
    "                'customer_id': customer['customer_id'],\n",
    "                'customer_name': customer['customer_name'],\n",
    "                'valid_from': customer['elt_file_date'],\n",
    "                'valid_to': datetime.strptime('9999-01-01', '%Y-%m-%d').strftime('%Y-%m-%d'),\n",
    "                'his_flag': 'C',\n",
    "                'elt_load_time': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }\n",
    "            # Append new customer data to DataFrame and insert into database\n",
    "            new_customer_df = pd.DataFrame([new_customer_data])\n",
    "            #new_customer_df.to_sql('customer', conn, index=False, if_exists='append', schema='silver')\n",
    "            insert_records(new_customer_df, 'customer', conn,'silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b8ea8-bd67-4e9e-b167-5c6d485a2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into segments using insertloading\n",
    "\n",
    "query = 'SELECT DISTINCT Segment FROM bronze.orders'\n",
    "segments = pd.read_sql(query, conn)\n",
    "\n",
    "silver_query = 'SELECT * FROM silver.segment'\n",
    "silver_segments = pd.read_sql(silver_query, conn)\n",
    "segments['sk_segment_id'] = segments['Segment'].apply(lambda x: consistent_hash(str(x), 10 ** 9))\n",
    "\n",
    "# Find new segment keys that don't exist in the silver table\n",
    "new_segment_keys = segments[~segments['sk_segment_id'].isin(silver_segments['sk_segment_id'])]\n",
    "\n",
    "# Prepare DataFrame for new segment keys\n",
    "new_segments_df = pd.DataFrame({\n",
    "    'sk_segment_id': new_segment_keys['sk_segment_id'],\n",
    "    'segment': new_segment_keys['Segment'],\n",
    "    'elt_load_time': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "})\n",
    "\n",
    "# Insert new segment keys into silver.segments table\n",
    "if not new_segments_df.empty:\n",
    "    #new_segments_df.to_sql('segment', conn, if_exists='append', index=False, schema='silver')\n",
    "    insert_records(new_segments_df, 'segment', conn,'silver')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e6a24-366e-495e-9c42-d5ac38d5281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load categories\n",
    "\n",
    "query = 'SELECT DISTINCT category FROM bronze.orders'\n",
    "category = pd.read_sql(query, conn)\n",
    "\n",
    "# Fetch from silver.segment\n",
    "silver_query_category = 'SELECT * FROM silver.category'\n",
    "silver_category = pd.read_sql(silver_query_category, conn)\n",
    "\n",
    "# Compute hash for values\n",
    "category['sk_category_id'] = category['category'].apply(lambda x: consistent_hash(str(x), 10 ** 9))\n",
    "\n",
    "# Find new keys that don't exist in the silver table\n",
    "new_category_keys = category[~category['sk_category_id'].isin(silver_category['sk_category_id'])]\n",
    "\n",
    "# Prepare DataFrame for new keys\n",
    "new_category_df = pd.DataFrame({\n",
    "    'sk_category_id': new_category_keys['sk_category_id'],\n",
    "    'category_name': new_category_keys['category'],\n",
    "    'elt_load_time': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "})\n",
    "\n",
    "# Insert new keys into silver.segments table\n",
    "if not new_category_df.empty:\n",
    "    #new_category_df.to_sql('category', conn, if_exists='append', index=False, schema='silver')\n",
    "    insert_records(new_category_df, 'category', conn, 'silver')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6c257-6fd6-4d04-9bc7-6a1819062b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Sub-categories\n",
    "\n",
    "querysubcategory = 'SELECT DISTINCT sub_category FROM bronze.orders'\n",
    "Subcategory = pd.read_sql(querysubcategory, conn)\n",
    "\n",
    "# Fetch from silver.sub_category\n",
    "silver_query_Subcategory = 'SELECT * FROM silver.sub_category'\n",
    "silver_Subcategory = pd.read_sql(silver_query_Subcategory, conn)\n",
    "\n",
    "# Compute hash for segment values to get sk_segment_id\n",
    "Subcategory['sk_sub_category_id'] = Subcategory['sub_category'].apply(lambda x: consistent_hash(str(x), 10 ** 9))\n",
    "new_Subcategory_keys = Subcategory[~Subcategory['sk_sub_category_id'].isin(silver_Subcategory['sk_sub_category_id'])]\n",
    "\n",
    "# Prepare DataFrame for new keys\n",
    "new_Subcategory_df = pd.DataFrame({\n",
    "    'sk_sub_category_id': new_Subcategory_keys['sk_sub_category_id'],\n",
    "    'sub_category_name': new_Subcategory_keys['sub_category'],\n",
    "    'elt_load_time': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "})\n",
    "\n",
    "# Insert new keys into silver.segments table\n",
    "if not new_Subcategory_df.empty:\n",
    "    #new_Subcategory_df.to_sql('sub_category', conn, if_exists='append', index=False, schema='silver')\n",
    "    insert_records(new_Subcategory_df, 'sub_category', conn, 'silver')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a00e2-368b-4fd1-9a87-76777926475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Shipment Type\n",
    "\n",
    "queryshipmode = 'SELECT DISTINCT ship_mode FROM bronze.orders'\n",
    "ShipmentMode = pd.read_sql(queryshipmode, conn)\n",
    "\n",
    "# Fetch from silver.sub_category\n",
    "silver_ShipmentMode = 'SELECT * FROM silver.shipment_mode'\n",
    "silver_ShipmentMode = pd.read_sql(silver_ShipmentMode, conn)\n",
    "\n",
    "# Compute hash for segment values to get sk_segment_id\n",
    "ShipmentMode['sk_shipment_mode_id'] = ShipmentMode['ship_mode'].apply(lambda x: consistent_hash(str(x), 10 ** 9))\n",
    "new_ShipmentMode_keys = ShipmentMode[~ShipmentMode['sk_shipment_mode_id'].isin(silver_ShipmentMode['sk_shipment_mode_id'])]\n",
    "\n",
    "# Prepare DataFrame for new keys\n",
    "new_ShipmentMode_df = pd.DataFrame({\n",
    "    'sk_shipment_mode_id': new_ShipmentMode_keys['sk_shipment_mode_id'],\n",
    "    'shipment_type': new_ShipmentMode_keys['ship_mode'],\n",
    "    'elt_load_time': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "})\n",
    "\n",
    "# Insert new keys into silver.segments table\n",
    "if not new_ShipmentMode_df.empty:\n",
    "    #new_ShipmentMode_df.to_sql('shipment_mode', conn, if_exists='append', index=False, schema='silver')\n",
    "    insert_records(new_ShipmentMode_df, 'shipment_mode', conn, 'silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605fc67-5250-4ebe-9d97-54937b547717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laod Location\n",
    "\n",
    "querylocation = 'SELECT DISTINCT country, city,state,postal_code,region FROM bronze.orders'\n",
    "location = pd.read_sql(querylocation, conn)\n",
    "\n",
    "# Fetch from silver.sub_category\n",
    "silver_location = 'SELECT * FROM silver.location'\n",
    "silver_location = pd.read_sql(silver_location, conn)\n",
    "\n",
    "# Compute hash for segment values to get sk_segment_id\n",
    "location['sk_location_id'] = location['postal_code'].apply(lambda x: consistent_hash(str(x), 10 ** 9))\n",
    "new_location_keys = location[~location['sk_location_id'].isin(silver_location['sk_location_id'])]\n",
    "\n",
    "# Prepare DataFrame for new keys\n",
    "new_location_df = pd.DataFrame({\n",
    "    'sk_location_id': new_location_keys['sk_location_id'],\n",
    "    'postal_code': new_location_keys['postal_code'],\n",
    "    'country': new_location_keys['country'],\n",
    "    'city': new_location_keys['city'],\n",
    "    'state': new_location_keys['state'],\n",
    "    'region': new_location_keys['region'],\n",
    "    'elt_load_time': pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "})\n",
    "\n",
    "# Insert new keys into silver.segments table\n",
    "if not new_location_df.empty:\n",
    "    #new_location_df.to_sql('location', conn, if_exists='append', index=False, schema='silver')\n",
    "    insert_records(new_location_df, 'location', conn, 'silver')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe1b4e-9a70-4c9b-adcd-72d60cb4fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Product SCD2\n",
    "\n",
    "from sqlalchemy import text\n",
    "queryproduct = \"\"\"SELECT * FROM \n",
    "                 (SELECT product_id,\n",
    "                product_name,\n",
    "                category, \n",
    "                sub_category,\n",
    "                elt_file_date, \n",
    "                ROW_NUMBER() OVER (PARTITION BY product_id, product_name ORDER BY elt_file_timestamp DESC) \n",
    "                RK FROM demo.bronze.orders WHERE elt_file_timestamp = (SELECT MAX(elt_file_timestamp) FROM demo.bronze.orders)) X WHERE RK = 1 \n",
    "                \"\"\"\n",
    "products = pd.read_sql(queryproduct, conn)\n",
    "     \n",
    "silverquery_product = \"SELECT * FROM silver.product\"\n",
    "existing_products = pd.read_sql(silverquery_product, conn)\n",
    "existing_products_filtered = existing_products.copy()\n",
    "\n",
    "for index, product in products.iterrows():\n",
    "    # Check if the product exists in the silver.product table\n",
    "    existing_product = existing_products_filtered[(existing_products_filtered['product_id'] == product['product_id'])]\n",
    "    if len(existing_product) == 0:    \n",
    "        sk_product_id = consistent_hash(product['product_id']) % (10 ** 9)\n",
    "        category_id = consistent_hash(product['category']) % (10 ** 9)\n",
    "        sub_category_id = consistent_hash(product['sub_category']) % (10 ** 9)\n",
    "        product_id = product['product_id']\n",
    "        product_name = product['product_name']\n",
    "        valid_from = product['elt_file_date']\n",
    "        valid_to = '9999-01-01'\n",
    "        his_flag = 'C'\n",
    "        elt_load_time = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        new_product_data = {\n",
    "            'sk_product_id': sk_product_id,\n",
    "            'category_id': category_id,\n",
    "            'product_id': product_id,\n",
    "            'sub_category_id': sub_category_id,\n",
    "            'product_name': product_name,\n",
    "            'valid_from': valid_from,\n",
    "            'valid_to': valid_to,\n",
    "            'his_flag': his_flag,\n",
    "            'elt_load_time': elt_load_time\n",
    "        }\n",
    "        \n",
    "        new_product_df = pd.DataFrame([new_product_data])\n",
    "        #new_product_df.to_sql('product', conn, index=False, if_exists='append', schema='silver')\n",
    "        insert_records(new_product_df, 'product', conn, 'silver')\n",
    "        \n",
    "    else:\n",
    "        # If the product exists, update its validity period if there's a change\n",
    "        existing_product = existing_product.iloc[0]\n",
    "        if (product['product_name'].lower() != existing_product['product_name'].lower()):\n",
    "            valid_to = product['elt_file_date']\n",
    "            his_flag = 'H'\n",
    "            product_id = product['product_id']\n",
    "            \n",
    "            update_query = f\"\"\"\n",
    "            UPDATE silver.product SET valid_to = '{valid_to}', \n",
    "            his_flag = '{his_flag}' \n",
    "            WHERE product_id = '{product_id}' and valid_to = '9999-01-01'\"\"\"\n",
    "            with conn.connect() as connection:\n",
    "                connection.execute(text(update_query))\n",
    "                connection.commit()\n",
    "\n",
    "            # Insert a new record for the updated product\n",
    "            sk_product_id = consistent_hash(product['product_id']) % (10 ** 9)\n",
    "            category_id = consistent_hash(product['category']) % (10 ** 9)\n",
    "            sub_category_id = consistent_hash(product['sub_category']) % (10 ** 9)\n",
    "            product_id = product['product_id']\n",
    "            product_name = product['product_name']\n",
    "            valid_from = product['elt_file_date']\n",
    "            valid_to = '9999-01-01'\n",
    "            his_flag = 'C'\n",
    "            elt_load_time = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            new_product_data = {\n",
    "                'sk_product_id': sk_product_id,\n",
    "                'category_id': category_id,\n",
    "                'product_id': product_id,\n",
    "                'sub_category_id': sub_category_id,\n",
    "                'product_name': product_name,\n",
    "                'valid_from': valid_from,\n",
    "                'valid_to': valid_to,\n",
    "                'his_flag': his_flag,\n",
    "                'elt_load_time': elt_load_time\n",
    "            }\n",
    "            \n",
    "            new_product_df = pd.DataFrame([new_product_data])\n",
    "            #new_product_df.to_sql('product', conn, index=False, if_exists='append', schema='silver')\n",
    "            insert_records(new_product_df, 'product', conn, 'silver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49de15-4b6c-45a5-b963-42093bc4e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fact Table\n",
    "\n",
    "import dateutil.parser\n",
    "from sqlalchemy import text\n",
    "\n",
    "Ordersquery = f\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        order_date,\n",
    "        ship_date,\n",
    "        ship_mode,\n",
    "        customer_id,\n",
    "        segment,\n",
    "        postal_code,\n",
    "        product_id,\n",
    "        sales,\n",
    "        quantity,\n",
    "        discount,\n",
    "        profit,\n",
    "        elt_file_date,\n",
    "        elt_file_timestamp\n",
    "    FROM \n",
    "        bronze.orders\n",
    "    WHERE \n",
    "        elt_file_timestamp = (SELECT MAX(elt_file_timestamp) FROM bronze.orders)\"\"\"\n",
    "\n",
    "orders = pd.read_sql(Ordersquery, conn)\n",
    "silverquery = \"SELECT * FROM silver.fact_order_transaction\"\n",
    "existing_orders = pd.read_sql(silverquery, conn)\n",
    "existing_orders_filtered = existing_orders.copy()\n",
    "\n",
    "for index, order in orders.iterrows():\n",
    "    existing_order = existing_orders_filtered[\n",
    "    (existing_orders_filtered['order_id'] == order['order_id']) & \n",
    "    (existing_orders_filtered['transaction_month'] == order['elt_file_date']) & \n",
    "    (existing_orders_filtered['product_id'] == hash(order['product_id']) % (10 ** 9))\n",
    "]\n",
    "    \n",
    "    if len(existing_order) == 0:\n",
    "        order_id = order['order_id']\n",
    "        ship_mode_id = hash(order['ship_mode']) % (10 ** 9)\n",
    "        location_id = hash(order['postal_code']) % (10 ** 9)\n",
    "        customer_id = hash(order['customer_id']) % (10 ** 9)\n",
    "        product_id = hash(order['product_id']) % (10 ** 9)\n",
    "        order_date = dateutil.parser.parse(order['order_date']).strftime(\"%Y-%m-%d\")\n",
    "        ship_date = dateutil.parser.parse(order['ship_date']).strftime(\"%Y-%m-%d\")\n",
    "        sales = order['sales']\n",
    "        quantity = order['quantity']\n",
    "        discount = order['discount']\n",
    "        profit = order['profit']\n",
    "        transaction_month = order['elt_file_date']\n",
    "        valid_from = order['elt_file_timestamp']\n",
    "        valid_to = datetime.strptime('9999-01-01 00:00:00', '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "        status = ''\n",
    "        elt_load_time = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        new_order_data = {\n",
    "            'order_id': order_id,\n",
    "            'ship_mode_id': ship_mode_id,\n",
    "            'location_id': location_id,\n",
    "            'product_id': product_id,\n",
    "            'customer_id': customer_id,\n",
    "            'order_date': order_date,\n",
    "            'ship_date': ship_date,\n",
    "            'sales': sales,\n",
    "            'quantity': quantity,\n",
    "            'discount': discount,\n",
    "            'profit': profit,\n",
    "            'transaction_month' :transaction_month,\n",
    "            'valid_from': valid_from,\n",
    "            'valid_to': valid_to,\n",
    "            'status': status,\n",
    "            'elt_load_time': elt_load_time\n",
    "        }\n",
    "        # Append new order data to DataFrame and insert into database\n",
    "        new_order_df = pd.DataFrame([new_order_data])\n",
    "        #new_order_df.to_sql('fact_order_transaction', conn, index=False, if_exists='append', schema='silver')\n",
    "        insert_records(new_order_df, 'fact_order_transaction', conn, 'silver')\n",
    "    else:\n",
    "        existing_order = existing_order.iloc[0] \n",
    "        if (existing_order['valid_from'] != order['elt_file_timestamp']):\n",
    "            # Update existing record to 'D'\n",
    "            existing_orders_filtered.loc[existing_orders_filtered['order_id'] == order['order_id'], 'status'] = 'D'\n",
    "            \n",
    "            # Update the database with the updated status\n",
    "            valid_to = order['elt_file_timestamp']\n",
    "            status = 'D'\n",
    "            order_id = order['order_id']\n",
    "            product_id = hash(order['product_id']) % (10 ** 9)\n",
    "            transaction_month = order['elt_file_date']\n",
    "            \n",
    "            # Update the database with the updated values\n",
    "            update_query = f\"\"\"\n",
    "            UPDATE silver.fact_order_transaction \n",
    "            SET valid_to = '{valid_to}' , status = '{status}' \n",
    "            WHERE order_id = '{order_id}' AND product_id = '{product_id}' AND transaction_month = '{transaction_month}'\"\"\"\n",
    "            with conn.connect() as connection:\n",
    "                connection.execute(text(update_query))\n",
    "                connection.commit()\n",
    "            \n",
    "            # Insert the new record\n",
    "            order_id = order['order_id']\n",
    "            ship_mode_id = hash(order['ship_mode']) % (10 ** 9)\n",
    "            customer_id = hash(order['customer_id']) % (10 ** 9)\n",
    "            location_id = hash(order['postal_code']) % (10 ** 9)\n",
    "            product_id = hash(order['product_id']) % (10 ** 9)\n",
    "            order_date = dateutil.parser.parse(order['order_date']).strftime(\"%Y-%m-%d\")\n",
    "            ship_date = dateutil.parser.parse(order['ship_date']).strftime(\"%Y-%m-%d\")\n",
    "            sales = order['sales']\n",
    "            quantity = order['quantity']\n",
    "            discount = order['discount']\n",
    "            profit = order['profit']\n",
    "            transaction_month = order['elt_file_date']\n",
    "            valid_from = order['elt_file_timestamp']\n",
    "            valid_to = datetime.strptime('9999-01-01 00:00:00', '%Y-%m-%d %H:%M:%S').strftime('%Y-%m-%d %H:%M:%S')\n",
    "            status = ''\n",
    "            elt_load_time = pd.Timestamp.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            new_order_data = {\n",
    "                'order_id': order_id,\n",
    "                'ship_mode_id': ship_mode_id,\n",
    "                'customer_id': customer_id,\n",
    "                'location_id': location_id,\n",
    "                'product_id': product_id,\n",
    "                'order_date': order_date,\n",
    "                'ship_date': ship_date,\n",
    "                'sales': sales,\n",
    "                'quantity': quantity,\n",
    "                'discount': discount,\n",
    "                'profit': profit,\n",
    "                'transaction_month' :transaction_month,\n",
    "                'valid_from': valid_from,\n",
    "                'valid_to': valid_to,\n",
    "                'status': status,\n",
    "                'elt_load_time': elt_load_time\n",
    "            }\n",
    "            # Append new order data to DataFrame and insert into database\n",
    "            new_order_df = pd.DataFrame([new_order_data])\n",
    "            insert_records(new_order_df, 'fact_order_transaction', conn, 'silver')\n",
    "            #new_order_df.to_sql('fact_order_transaction', conn, index=False, if_exists='append', schema='silver')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
